{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Matching\n",
    "\n",
    "In this notebook we are going to build a tool that taken as input a medical prescription in pdf,jpg format or a string gives me as output the list of facilities that perform medical analysis.\n",
    "\n",
    "To extract information from prescriptions we will use OCR technology while to match the words in the prescription with those in the dataset we will use the fuzzywuzzy library.\n",
    "\n",
    "OCR pdf: https://pypi.org/project/PyPDF2/\n",
    "\n",
    "OCR jpg: https://www.jaided.ai/easyocr/tutorial/\n",
    "\n",
    "FuzzyWuzzy: https://pypi.org/project/fuzzywuzzy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install  pypdf langchain tika typing fuzzywuzzy pymupdf easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from tika import parser\n",
    "from typing import List\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import fitz\n",
    "import easyocr\n",
    "import numpy as np\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://github.com/SimoneFarallo/public_and_social_services/raw/main/data/final_data_cleaned.csv')\n",
    "dataset = dataset.drop(dataset.columns[0], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Matching for pdf\n",
    "\n",
    "I create a function that taken as input a pdf, gives me as output a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to match  prescriptions with a dataset \n",
    "def match_recipe_to_dataset_pdf(pdf_file: str, dataset: pd.DataFrame, threshold: int = 90) -> List[Tuple]:\n",
    "\n",
    "    # Check if pdf_file is a string, if not raise TypeError\n",
    "    if not isinstance(pdf_file, str):\n",
    "        raise TypeError(\"The PDF file path must be a string.\")\n",
    "    \n",
    "    # Check if dataset is a pandas dataframe, if not raise TypeError\n",
    "    if not isinstance(dataset, pd.DataFrame):\n",
    "        raise TypeError(\"The dataset must be a pandas.DataFrame object.\")\n",
    "\n",
    "    # Extract text from pdf file using fitz library\n",
    "    pdf_file = fitz.open(pdf_file)\n",
    "    text = ''\n",
    "    for page in pdf_file:\n",
    "        text += page.get_text()\n",
    "\n",
    "    # Find words of interest using regex \n",
    "    words = re.findall(r' - ([A-Za-z\\s]+)', text) # Initial regex function to extract words from recipe (starting with ' - ([A-Z]+))'\n",
    "\n",
    "    # Match words with words in dataset 'Codice prestazione ambulatoriale' column using fuzzywuzzy\n",
    "    matches = []\n",
    "    for word in words:\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError(\"The word of interest must be a string.\") # Function to raise error for non-string input\n",
    "        scores = process.extract(word, dataset['Codice prestazione ambulatoriale'], scorer=fuzz.token_set_ratio, limit=1) # Use fuzzywuzzy library to score the similarity between two strings \n",
    "        filtered_scores = [score for score in scores if score[1] >= threshold] # Filter scores based on the threshold value\n",
    "        for score in filtered_scores:\n",
    "            rows = dataset[dataset['Codice prestazione ambulatoriale'] == score[0]].iterrows() # Select the output rows based on the matching score\n",
    "            matches.extend([(word, row[1], score[1]) for row in rows]) # Add the matched words, output rows, and score to the list 'matches'\n",
    "\n",
    "    # Sort the results based on score\n",
    "    matches_sorted = sorted(matches, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Return a list of tuples containing matched word, output row, and score\n",
    "    return matches_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:\\Users\\Simone\\Documents\\Desktop\\public_and_social_services\\ricette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_recipe_to_dataset_pdf('ricetta_1.pdf',dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Matching for png\n",
    "\n",
    "I create a function that taken as input a png, gives me as output a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(['it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to match  prescriptions with a dataset from png\n",
    "def match_recipe_to_dataset_jpg(file: str, dataset: pd.DataFrame, threshold: int = 90) -> List[Tuple]:\n",
    "    # Check the input arguments types\n",
    "    if not isinstance(file, str):\n",
    "        raise TypeError(\"The PDF file path must be a string.\")\n",
    "    if not isinstance(dataset, pd.DataFrame):\n",
    "        raise TypeError(\"The dataset must be a pandas.DataFrame object.\")\n",
    "    \n",
    "    # Set-up the reader object and extract the text from the PDF file\n",
    "    recipe = reader.readtext(file)\n",
    "    result_string = \"\"\n",
    "    for i in range(len(recipe)):\n",
    "        element = recipe[i][1] # strings are in this position\n",
    "        result_string += element\n",
    "        result_string += \" \"\n",
    "   \n",
    "    # Clean the text by removing the dots\n",
    "    result_string = result_string.replace(\".\",\"\") \n",
    "    # Set-up the regex for matching codes and descriptions\n",
    "    regex = r\"\\((\\d\\w*)\\)([A-Z ]+)\"\n",
    "    matches = re.findall(regex,result_string)\n",
    "    output = []\n",
    "    # Extract the descriptions from the matches\n",
    "    for match in matches:\n",
    "      output.append(match[1])\n",
    "\n",
    "    # Save the relevant words to match them with the 'Codice prestazione ambulatoriale' column\n",
    "    words = output \n",
    "\n",
    "    # Match the words with the dataset using fuzzywuzzy\n",
    "    matches = []\n",
    "    for word in words:\n",
    "        # Check the input argument type\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError(\"The word of interest must be a string.\")\n",
    "        # Match the word with the 'Codice prestazione ambulatoriale' column using fuzzywuzzy\n",
    "        scores = process.extract(word, dataset['Codice prestazione ambulatoriale'], scorer=fuzz.token_set_ratio, limit=1) # Here we perform fuzzy matching; the 'limit' parameter limits the number of matching results\n",
    "        # Filter out the results below the threshold\n",
    "        filtered_scores = [score for score in scores if score[1] >= threshold]\n",
    "        for score in filtered_scores:\n",
    "            # Identify the rows based on the matching code\n",
    "            rows = dataset[dataset['Codice prestazione ambulatoriale'] == score[0]].iterrows()\n",
    "            # Append the matches to the output list\n",
    "            matches.extend([(word, row[1], score[1]) for row in rows])\n",
    "\n",
    "    # Sort the matches in descending order based on the score\n",
    "    matches_sorted = sorted(matches, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return matches_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_recipe_to_dataset_jpg('ricetta_2.png', dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching tool\n",
    "\n",
    "Taking the functions created earlier as a reference, we create a function that takes both pdf and png as input, also modify the output to be a dataframe so that it can be filtered later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to match  prescriptions with a dataset and create a new dataframe\n",
    "def match_recipe_to_dataset_and_create_df(file: str, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    # raise error if file path is not a string\n",
    "    if not isinstance(file, str):\n",
    "        raise TypeError(\"File path must be a string.\")\n",
    "    # raise error if dataset is not a pandas DataFrame\n",
    "    if not isinstance(dataset, pd.DataFrame):\n",
    "        raise TypeError(\"Dataset must be a pandas.DataFrame.\")\n",
    "\n",
    "    # if file is a pdf, extract text from file\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_file = fitz.open(file) # open pdf file\n",
    "        text = ''\n",
    "        for page in pdf_file:\n",
    "            text += page.get_text() # extract text from each page\n",
    "\n",
    "        # identify words of interest using regular expression\n",
    "        words = re.findall(r' - ([A-Za-z\\s]+)', text)\n",
    "\n",
    "    # if file is a png, read text from image using OCR\n",
    "    elif file.endswith(\".png\"):\n",
    "        ricetta = reader.readtext(file) # use tesseract to read text\n",
    "        result_string = \"\"\n",
    "        for i in range(len(ricetta)):\n",
    "            elemento = ricetta[i][1] #string values are stored in this position in the tuple\n",
    "            result_string += elemento # concatenate all strings\n",
    "            result_string += \" \" # add space between each string\n",
    "        \n",
    "        result_string = result_string.replace(\".\",\"\") # remove dots\n",
    "        regex = r\"\\((\\d\\w*)\\)([A-Z ]+)\"\n",
    "        matches = re.findall(regex, result_string) # identify string patterns using regex\n",
    "        output = []\n",
    "        for match in matches:\n",
    "            output.append(match[1]) # append string to output list\n",
    "\n",
    "        words = output\n",
    "\n",
    "    # raise error if file format is not supported\n",
    "    else:\n",
    "        raise ValueError(\"File format not supported. Please use a pdf or png file.\")\n",
    "\n",
    "    # match words with values in Codice prestazione ambulatoriale column of dataset using fuzzywuzzy library\n",
    "    matches = []\n",
    "    for word in words:\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError(\"Word of interest must be a string.\")\n",
    "        scores = process.extract(word, dataset['Codice prestazione ambulatoriale'], scorer=fuzz.token_set_ratio, limit=1)\n",
    "        filtered_scores = [score for score in scores if score[1] >= 90]\n",
    "        for score in filtered_scores:\n",
    "            rows = dataset[dataset['Codice prestazione ambulatoriale'] == score[0]].iterrows()\n",
    "            matches.extend([(word,row[1], score[1]) for row in rows])\n",
    "\n",
    "    # sort matches by score in descending order\n",
    "    matches_sorted = sorted(matches, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # create empty dictionary to populate with data\n",
    "    data = {}\n",
    "\n",
    "    # iterate through list of tuples\n",
    "    for row in matches_sorted:\n",
    "        # extract category name and associated row data\n",
    "        category, row_data, _ = row\n",
    "        # iterate through columns in row data\n",
    "        for col, val in row_data.items():\n",
    "            # create key in dictionary if it doesn't exist already\n",
    "            if col not in data:\n",
    "                data[col] = []\n",
    "            # append value to list of values associated with key\n",
    "            data[col].append(val)\n",
    "        # if a column is missing from current row, add a None value\n",
    "        for col in data.keys():\n",
    "            if col not in row_data:\n",
    "                data[col].append(None)\n",
    "\n",
    "    # create new DataFrame from dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_recipe_to_dataset_and_create_df('ricetta_3.png', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ricetta = match_recipe_to_dataset_and_create_df('ricetta_3.pdf', dataset)\n",
    "output_ricetta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter the dataset\n",
    "#Is possible add more columns to filter, for now we use only 2\n",
    "def filter_data(data_frame, comune=None, risposta_strutt= None):\n",
    "    if comune is not None and risposta_strutt is not None:\n",
    "        filtered_df = data_frame[(data_frame['Comune struttura'] == comune) & (data_frame['Struttura privata'] == risposta_strutt)]\n",
    "    elif comune is not None:\n",
    "        filtered_df = data_frame[data_frame['Comune struttura'] == comune]\n",
    "    elif risposta_strutt is not None:\n",
    "        filtered_df = data_frame[data_frame['Struttura privata'] == risposta_strutt]\n",
    "    else:\n",
    "        filtered_df = data_frame.copy()\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data(output_ricetta,'BERGAMO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to filter for Comune = Milano and Struttre private = No\n",
    "filter_data(output_ricetta,'BERGAMO','Sì')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset with strings\n",
    "\n",
    "This function does the same thing as the others, but as input it takes a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to search structures directly with performance names\n",
    "def search_words(words):\n",
    "    matches = []\n",
    "    data = {}\n",
    "\n",
    "    for word in words:\n",
    "        if not isinstance(word, str): #Check if the word is a string, otherwise raise a TypeError\n",
    "            raise TypeError(\"The word of interest must be a string.\")\n",
    "        scores = process.extract(word, dataset['Codice prestazione ambulatoriale'], scorer=fuzz.token_set_ratio, limit=1) #Extract matching scores between the word and the dataset\n",
    "        filtered_scores = [score for score in scores if score[1] >= 70] #Filter scores based on a threshold of 70%\n",
    "        for score in filtered_scores:\n",
    "            rows = dataset[dataset['Codice prestazione ambulatoriale'] == score[0]].iterrows() #Get the rows of the dataset that match the word\n",
    "            matches.extend([(word,row[1], score[1]) for row in rows]) #Extend the matches list with a tuple of the word, the row data, and the matching score\n",
    "            for row in matches:\n",
    "                category, row_data, _ = row\n",
    "                for col, val in row_data.items():\n",
    "                    if col not in data:\n",
    "                        data[col] =[]\n",
    "                    data[col].append(val) #Add the values of therow to the respective column in the data dictionary\n",
    "                for col in data.keys():\n",
    "                    if col not in row_data:\n",
    "                        data[col].append(None) #If a column is missing in the row, add None as a placeholder\n",
    "\n",
    "    df = pd.DataFrame(data) #Create a pandas DataFrame from the data dictionary\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_query = search_words(['calcitonina','emocromo'])\n",
    "output_query.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to filter for Bergamo and private strcture\n",
    "filter_data(output_query,'BERGAMO','Sì')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interfaccia per ocr tool\n",
    "def match_recipe_to_dataset_interface(\n",
    "    file: gr.inputs.File,\n",
    "    comune: str = None,\n",
    "    risposta_strutt: str = None\n",
    "):\n",
    "    # Ottieni il percorso del file\n",
    "    file_path = file.name\n",
    "\n",
    "    # Esegui la tua funzione originale con il percorso del file (stringa) e il DataFrame\n",
    "    df = match_recipe_to_dataset_and_create_df(file_path, dataset)\n",
    "\n",
    "    # Filtra il DataFrame se i parametri di filtraggio sono specificati\n",
    "    if comune is not None or risposta_strutt is not None:\n",
    "        df = filter_data(df, comune, risposta_strutt)\n",
    "\n",
    "    # Restituisci il risultato come output dell'interfaccia\n",
    "    return df\n",
    "\n",
    "# Definisci gli input per l'interfaccia utente\n",
    "file_input = gr.inputs.File(label=\"File\")\n",
    "comune_input = gr.inputs.Textbox(label=\"Comune\")\n",
    "risposta_strutt_input = gr.inputs.Textbox(label=\"Struttura privata\")\n",
    "\n",
    "# Definisci l'interfaccia utente utilizzando la tua funzione di Gradio e gli input definiti\n",
    "interface = gr.Interface(\n",
    "    fn=match_recipe_to_dataset_interface,\n",
    "    inputs=[file_input, comune_input, risposta_strutt_input],\n",
    "    outputs=[\"dataframe\"],\n",
    "    title=\"Matching Recipe to Dataset\",\n",
    "    description=\"Insert here your recipe and discover structure in Lombardy.\",\n",
    "    outputs_labels=[\"Matched Data\", \"Filtered Data\"]\n",
    ")\n",
    "\n",
    "# Avvia l'interfaccia utente\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_words_interface(words):\n",
    "    try:\n",
    "        df = search_words(words)\n",
    "        unique_values = df\n",
    "        return unique_values\n",
    "    except TypeError as e:\n",
    "        return str(e)\n",
    "\n",
    "# Creazione dell'interfaccia Gradio\n",
    "iface = gr.Interface(fn=search_words_interface, inputs=\"text\", outputs=\"dataframe\")\n",
    "\n",
    "# Avvia l'interfaccia Gradio\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simone\\AppData\\Local\\Temp\\ipykernel_29608\\2545255791.py:1: UserWarning: You have unused kwarg parameters in Interface, please remove them: {'debug': True}\n",
      "  demo = gr.Interface(fn=search_words, inputs=\"text\", outputs=\"dataframe\",debug=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '\n",
      "']\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(fn=search_words, inputs=\"text\", outputs=\"dataframe\",debug=True)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
